{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "codex-attention.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7B76Yd0xbgXbosAlU9YhX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ell-hol/stonks-wid-codex/blob/main/codex_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQjQmKSglkLO"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim: int) -> None:\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self._dim = dim\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        keys: Tensor,\n",
        "        values: Tensor,\n",
        "        mask: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        # score = Softmax(Q.K / sqrt(dim)) * V\n",
        "        score = torch.matmul(query, keys.transpose(1, 2))\n",
        "        score = score / (self._dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        score = F.softmax(score, dim=-1)\n",
        "\n",
        "        return torch.matmul(score, values)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim: int,\n",
        "        head_count: int,\n",
        "        dim_per_head: int,\n",
        "        dropout: float = 0.0\n",
        "    ) -> None:\n",
        "\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self._query_linear = nn.Linear(in_dim, head_count * dim_per_head)\n",
        "        self._keys_linear = nn.Linear(in_dim, head_count * dim_per_head)\n",
        "        self._value_linear = nn.Linear(in_dim, head_count * dim_per_head)\n",
        "\n",
        "        self._multi_head_attention = Attention(dim_per_head)\n",
        "        self._linear_final = nn.Linear(head_count * dim_per_head, in_dim)\n",
        "        self._dropout = nn.Dropout(dropout)\n",
        "        self._layer_norm = nn.LayerNorm(in_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        query: Tensor,\n",
        "        keys: Tensor,\n",
        "        values: Tensor,\n",
        "        mask: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self._query_linear(query).view(batch_size, -1, self._multi_head_attention._dim)\n",
        "        keys = self._keys_linear(keys).view(batch_size, -1, self._multi_head_attention._dim)\n",
        "        values = self._value_linear(values).view(batch_size, -1, self._multi_head_attention._dim)\n",
        "\n",
        "        query = query.transpose(0, 1)\n",
        "        keys = keys.transpose(0, 1)\n",
        "        values = values.transpose(0, 1)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        context = self._multi_head_attention(query, keys, values, mask)\n",
        "        context = context.transpose(0, 1)\n",
        "\n",
        "        output = self._linear_final(context)\n",
        "        output = self._dropout(output)\n",
        "\n",
        "        # residual connection\n",
        "        output = self._layer_norm(query + output)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqTdC99ima4a"
      },
      "source": [
        "\"\"\"\n",
        "Define a Transformer model that uses the defined multi-headed attention module\n",
        "\"\"\"\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_vocab: int,\n",
        "        dim_embedding: int,\n",
        "        dim_model: int,\n",
        "        dim_ff: int,\n",
        "        head_count: int,\n",
        "        n_layers: int,\n",
        "        max_len: int,\n",
        "        dropout: float = 0.0\n",
        "    ) -> None:\n",
        "\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self._n_vocab = n_vocab\n",
        "        self._dim_embedding = dim_embedding\n",
        "        self._dim_model = dim_model\n",
        "        self._dim_ff = dim_ff\n",
        "        self._head_count = head_count\n",
        "        self._n_layers = n_layers\n",
        "        self._max_len = max_len\n",
        "\n",
        "        self._embedding = nn.Embedding(n_vocab, dim_embedding)\n",
        "\n",
        "        self._postion_embedding = nn.Embedding(self._max_len, dim_model)\n",
        "        self._postion_embedding.weight.data = self.position_encoding_init(self._max_len, self._dim_model)\n",
        "\n",
        "        self._encoder = nn.Sequential(\n",
        "            *[\n",
        "                Encoder(dim_model, dim_ff, head_count, dropout, dim_per_head=16)\n",
        "                for _ in range(self._n_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self._decoder = nn.Sequential(\n",
        "            *[\n",
        "                Decoder(dim_model, dim_ff, head_count, dropout)\n",
        "                for _ in range(self._n_layers)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self._linear = nn.Linear(dim_model, n_vocab)\n",
        "\n",
        "        self._layer_norm = nn.LayerNorm(dim_model)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    @staticmethod\n",
        "    def position_encoding_init(n_position: int, dim_pos_embed: int) -> Tensor:\n",
        "\n",
        "        encoding = np.array([\n",
        "            [pos / np.power(10000, 2.0 * (j // 2) / dim_pos_embed) for j in range(dim_pos_embed)]\n",
        "            if pos != 0 else np.zeros(dim_pos_embed) for pos in range(n_position)\n",
        "        ])\n",
        "\n",
        "        encoding[1:, 0::2] = np.sin(encoding[1:, 0::2])\n",
        "        encoding[1:, 1::2] = np.cos(encoding[1:, 1::2])\n",
        "\n",
        "        return torch.tensor(encoding.astype('float32'))\n",
        "\n",
        "    def mask_pad(self, x: Tensor) -> Tensor:\n",
        "\n",
        "        return (x != 0).unsqueeze(-2)\n",
        "\n",
        "    def forward(self, x: Tensor, mask: Tensor) -> Tensor:\n",
        "\n",
        "        batch_size, _ = x.size()\n",
        "\n",
        "        pos = torch.arange(0, self._max_len).repeat(batch_size, 1)\n",
        "        mask_source = mask.clone().fill_(0)\n",
        "\n",
        "        x = self._embedding(x) + self._postion_embedding(pos)\n",
        "\n",
        "        for encoder in self._encoder:\n",
        "            x = encoder(x, mask, mask_source)\n",
        "\n",
        "        mask = mask_source\n",
        "\n",
        "        for decoder in self._decoder:\n",
        "            x = decoder(x, mask)\n",
        "\n",
        "        return self._linear(self._layer_norm(x))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOOus8MYs2r7"
      },
      "source": [
        "\"\"\"\n",
        "Define the Encoder classe used in the Transformer class.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model: int,\n",
        "        dim_per_head: int,\n",
        "        dim_ff: int,\n",
        "        head_count: int,\n",
        "        dropout: float = 0.0\n",
        "    ) -> None:\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._self_attention = MultiHeadAttention(dim_model, head_count, dim_per_head=dim_per_head)\n",
        "        self._feed_forward = PositionwiseFeedForward(dim_model, dim_ff, dropout)\n",
        "        self._layer_norm1 = nn.LayerNorm(dim_model)\n",
        "        self._layer_norm2 = nn.LayerNorm(dim_model)\n",
        "        self._dropout1 = nn.Dropout(dropout)\n",
        "        self._dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        mask: Tensor,\n",
        "        mask_source: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        query = self._layer_norm1(x)\n",
        "        query = self._self_attention(query, query, query, mask)\n",
        "        query = self._dropout1(query)\n",
        "        query += x\n",
        "\n",
        "        query_ff = self._layer_norm2(query)\n",
        "        query_ff = self._feed_forward(query_ff)\n",
        "        query_ff = self._dropout2(query_ff)\n",
        "        query += query_ff\n",
        "\n",
        "        return query\n",
        "\"\"\"\n",
        "Do the same for the Decoder class.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_model: int,\n",
        "        dim_ff: int,\n",
        "        head_count: int,\n",
        "        dropout: float = 0.0\n",
        "    ) -> None:\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._self_attention = MultiHeadAttention(dim_model, head_count)\n",
        "        self._encoder_decoder_attention = MultiHeadAttention(dim_model, head_count)\n",
        "        self._feed_forward = PositionwiseFeedForward(dim_model, dim_ff, dropout)\n",
        "        self._layer_norm1 = nn.LayerNorm(dim_model)\n",
        "        self._layer_norm2 = nn.LayerNorm(dim_model)\n",
        "        self._layer_norm3 = nn.LayerNorm(dim_model)\n",
        "        self._dropout1 = nn.Dropout(dropout)\n",
        "        self._dropout2 = nn.Dropout(dropout)\n",
        "        self._dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        mask: Tensor,\n",
        "        mask_source: Tensor = None\n",
        "    ) -> Tensor:\n",
        "\n",
        "        query = self._layer_norm1(x)\n",
        "        query = self._self_attention(query, query, query, mask)\n",
        "        query = self._dropout1(query)\n",
        "        query += x\n",
        "\n",
        "        x = self._layer_norm2(query)\n",
        "        x = self._encoder_decoder_attention(x, mask_source, mask_source)\n",
        "        x = self._dropout2(x)\n",
        "        x += query\n",
        "\n",
        "        query_ff = self._layer_norm3(x)\n",
        "        query_ff = self._feed_forward(query_ff)\n",
        "        query_ff = self._dropout3(query_ff)\n",
        "        x += query_ff\n",
        "\n",
        "        return x\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "hAzmowYyooy5",
        "outputId": "d9dbe356-b7e2-4f2f-da2e-617635deb23b"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch import Tensor\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def accuracy(y_true: Tensor, y_pred: Tensor) -> float:\n",
        "\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    return np.sum(y_true == y_pred) / len(y_true)\n",
        "\n",
        "\n",
        "def precision(y_true: Tensor, y_pred: Tensor, average: str = 'macro') -> float:\n",
        "\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    return sklearn.metrics.precision_score(y_true, y_pred, average=average)\n",
        "\n",
        "\n",
        "def recall(y_true: Tensor, y_pred: Tensor, average: str = 'macro') -> float:\n",
        "\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    return sklearn.metrics.recall_score(y_true, y_pred, average=average)\n",
        "\n",
        "\n",
        "def f1(y_true: Tensor, y_pred: Tensor, average: str = 'macro') -> float:\n",
        "\n",
        "    p = precision(y_true, y_pred, average=average)\n",
        "    r = recall(y_true, y_pred, average=average)\n",
        "\n",
        "    return 2 * p * r / (p + r)\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.CIFAR10(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transforms,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset=dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "model = Transformer(\n",
        "    n_vocab=10,\n",
        "    dim_embedding=512,\n",
        "    dim_model=512,\n",
        "    dim_ff=2048,\n",
        "    head_count=8,\n",
        "    n_layers=6,\n",
        "    max_len=32,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = CrossEntropyLoss()\n",
        "optimizer = Adam(params=model.parameters())\n",
        "\n",
        "epochs = 100\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "\n",
        "    avg_loss = 0\n",
        "    avg_acc = 0\n",
        "    avg_precision = 0\n",
        "    avg_recall = 0\n",
        "    avg_f1 = 0\n",
        "\n",
        "    for batch in loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x = batch[0].to(device)\n",
        "        y = batch[1].to(device)\n",
        "\n",
        "        mask = model.mask_pad(x)\n",
        "        y_pred = model(x, mask)\n",
        "\n",
        "        loss = criterion(y_pred.view(-1, y_pred.size(-1)), y.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        avg_loss += loss.item() / len(loader)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            y = y.cpu().numpy()\n",
        "            y_pred = torch.argmax(y_pred, axis=-1).cpu().numpy()\n",
        "\n",
        "            avg_acc += accuracy(y, y_pred) / len(loader)\n",
        "            avg_precision += precision(y, y_pred) / len(loader)\n",
        "            avg_recall += recall(y, y_pred) / len(loader)\n",
        "            avg_f1 += f1(y, y_pred) / len(loader)\n",
        "\n",
        "    print('[%d / %d] loss: %.4f acc: %.4f p: %.4f r: %.4f f1: %.4f' % (epoch + 1, epochs, avg_loss, avg_acc, avg_precision, avg_recall, avg_f1))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-7f033a753cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mn_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m ).to(device)\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-17d6170195da>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_vocab, dim_embedding, dim_model, dim_ff, head_count, n_layers, max_len, dropout)\u001b[0m\n\u001b[1;32m     33\u001b[0m             *[\n\u001b[1;32m     34\u001b[0m                 \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             ]\n\u001b[1;32m     37\u001b[0m         )\n",
            "\u001b[0;32m<ipython-input-12-17d6170195da>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m             *[\n\u001b[1;32m     34\u001b[0m                 \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_per_head\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             ]\n\u001b[1;32m     37\u001b[0m         )\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'dim_per_head'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmkJmV3rr4Zb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}