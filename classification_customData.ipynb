{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_customData.ipynb",
      "provenance": [],
      "mount_file_id": "11eBGhDGoSLEgGehSDVSZZve2il3Yt3As",
      "authorship_tag": "ABX9TyPU3wwnGpoQmbO4pDxPZivz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ell-hol/stonks-wid-codex/blob/main/classification_customData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SEU16teBq3H"
      },
      "source": [
        "\"\"\"\n",
        "A simple classification model based on a pretrained ResNet18 backbone.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision.models import resnet18\n",
        "\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.resnet = resnet18(pretrained=True)\n",
        "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, n_classes)\n",
        "        # self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3,\n",
        "        #                               bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "A function to train the defined ResNet18 model on a custom Dataset.\n",
        "The custom Dataset is built from directories where each directory is a class.\n",
        "\"\"\"\n",
        "\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.transforms import Compose\n",
        "from torchvision.transforms import Normalize\n",
        "from torchvision.transforms import Resize\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "from torch.optim import Adam\n",
        "from torch.optim import SGD\n",
        "\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "def train_model(dataset_dir,\n",
        "                test_size=0.2,\n",
        "                batch_size=32,\n",
        "                num_workers=4,\n",
        "                num_epochs=100,\n",
        "                lr=0.001,\n",
        "                exp_name=None,\n",
        "                checkpoint_model=False,\n",
        "                checkpoint_interval=10,\n",
        "                use_gpu=False):\n",
        "    \"\"\"\n",
        "    Trains a ResNet18 model on the specified dataset.\n",
        "\n",
        "    Args:\n",
        "        dataset_dir (str): The path to the dataset\n",
        "        test_size (float): The size of the test set\n",
        "        batch_size (int): The batch size\n",
        "        num_workers (int): The number of worker threads to use for loading data\n",
        "        num_epochs (int): The number of epochs to train for\n",
        "        lr (float): The learning rate\n",
        "        exp_name (str): The name of the experiment (for logging)\n",
        "        checkpoint_model (bool): If True, save a checkpoint after each epoch\n",
        "        checkpoint_interval (int): If checkpointing, save a checkpoint every\n",
        "            `checkpoint_interval` epochs\n",
        "        use_gpu (bool): If True, attempt to train on the GPU\n",
        "\n",
        "    Returns:\n",
        "        (torch.nn.Module, torch.optim.Optimizer, dict): A tuple containing the\n",
        "            trained model, the optimizer, and a dictionary of metrics.\n",
        "\n",
        "    \"\"\"\n",
        "    # Create the output directory\n",
        "    os.makedirs(\"logs\", exist_ok=True)\n",
        "\n",
        "    # If no experiment name provided, make one\n",
        "    if exp_name is None:\n",
        "        exp_name = \"exp_\" + str(len(os.listdir(\"logs/\"))).zfill(3)\n",
        "        print(\"INFO: Using experiment name:\", exp_name)\n",
        "\n",
        "    # Prepare the dataset\n",
        "    dataset = ImageFolder(dataset_dir,\n",
        "                          transform=Compose([Resize((224, 224)), ToTensor()]))\n",
        "\n",
        "    # Split the dataset into train and test sets\n",
        "    train_size = int((1.0 - test_size) * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = \\\n",
        "        torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    # Encode the class labels\n",
        "    le = LabelEncoder()\n",
        "    le.fit(train_dataset.dataset.classes)\n",
        "\n",
        "    # Create data loaders for training and test sets\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              batch_size=batch_size,\n",
        "                              num_workers=num_workers,\n",
        "                              shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             num_workers=num_workers,\n",
        "                             shuffle=False)\n",
        "\n",
        "    # Prepare the model\n",
        "    model = ResNet18(len(le.classes_))\n",
        "    if use_gpu:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Prepare the optimizer\n",
        "    optimizer = Adam(model.parameters(), lr=lr)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[75, 90, 95], gamma=0.1)\n",
        "\n",
        "    # Set up the metrics\n",
        "    metrics = {\n",
        "        \"loss\": {},\n",
        "        \"acc\": {},\n",
        "        \"confusion\": {},\n",
        "        \"hist\": {}\n",
        "    }\n",
        "\n",
        "    # Train the model\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"=\" * 10, \"EPOCH {}/{}\".format(epoch + 1, num_epochs), \"=\" * 10)\n",
        "\n",
        "        # Train the model for one epoch\n",
        "        metrics = train(model, epoch, train_loader, optimizer, use_gpu, metrics)\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        metrics = evaluate(model, epoch, test_loader, use_gpu, metrics)\n",
        "\n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Logging\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"\\n\\nLoss:\", metrics[\"loss\"][epoch])\n",
        "            print(\"Accuracy:\", metrics[\"acc\"][epoch])\n",
        "\n",
        "            print(\"\\nConfusion Matrix\")\n",
        "            print(metrics[\"confusion\"][epoch])\n",
        "\n",
        "            # print(\"\\nHistogram\")\n",
        "            # print(metrics[\"hist\"][epoch])\n",
        "\n",
        "        # # Plot the metrics\n",
        "        # fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
        "        # axes[0].set_title(\"Loss\")\n",
        "        # axes[0].plot(metrics[\"loss\"])\n",
        "        # axes[0].grid(True)\n",
        "\n",
        "        # axes[1].set_title(\"Accuracy\")\n",
        "        # axes[1].plot(metrics[\"acc\"])\n",
        "        # axes[1].grid(True)\n",
        "\n",
        "        # # axes[2].set_title(\"Histogram\")\n",
        "        # # axes[2].plot(metrics[\"hist\"])\n",
        "        # # axes[2].grid(True)\n",
        "\n",
        "        # plt.savefig(\"logs/\" + exp_name + \"/metrics.png\")\n",
        "        # plt.close(fig)\n",
        "\n",
        "        # Save a checkpoint\n",
        "        if checkpoint_model and (epoch % checkpoint_interval == 0):\n",
        "            checkpoint_path = \\\n",
        "                \"logs/\" + exp_name + \"/checkpoint_\" + str(epoch) + \".pth\"\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                \"loss\": metrics[\"loss\"][epoch],\n",
        "                \"acc\": metrics[\"acc\"][epoch],\n",
        "                \"confusion\": metrics[\"confusion\"][epoch]\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"logs/\" + exp_name + \"/model.pth\")\n",
        "\n",
        "    # Save the label encoder\n",
        "    le_path = \"logs/\" + exp_name + \"/le.npy\"\n",
        "    np.save(le_path, le.classes_)\n",
        "\n",
        "    return model, optimizer, metrics"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLmQvdKwFzVH"
      },
      "source": [
        "def train(model, epoch, data_loader, optimizer, use_gpu, metrics):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to train.\n",
        "        data_loader (torch.utils.data.DataLoader): The training dataset.\n",
        "        optimizer (torch.optim.Optimizer): The optimizer.\n",
        "        use_gpu (bool): If True, attempt to train on the GPU.\n",
        "        metrics (dict): The dict where the metrics are stored.\n",
        "\n",
        "    Returns:\n",
        "        dict: The updated metrics dictionary.\n",
        "\n",
        "    \"\"\"\n",
        "    # Prepare the metrics dictionary\n",
        "    train_loss = []\n",
        "    train_acc = []\n",
        "    confusion = np.zeros((4, 4), dtype=int)\n",
        "\n",
        "    # Set the model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Iterate over training batches\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        # Prepare the inputs\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward propagation\n",
        "        logits = model(inputs)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # Back propagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute the total number of correct classifications\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        batch_size = inputs.size(0)\n",
        "        num_correct = (predicted == targets).sum().item()\n",
        "        batch_acc = num_correct / batch_size\n",
        "\n",
        "        # Update metrics\n",
        "        train_loss.append(loss.item())\n",
        "        train_acc.append(batch_acc)\n",
        "        confusion += confusion_matrix(targets.cpu().numpy(), predicted.cpu().numpy(), labels=[0, 1, 2, 3])\n",
        "\n",
        "        # Print the results on the console\n",
        "        print(\"\\r[Epoch {}] [Batch {} / {}] [Loss: {:.4f}] [Acc: {:.2f}%]\".format(\n",
        "            epoch + 1,\n",
        "            i + 1,\n",
        "            len(data_loader),\n",
        "            loss.item(),\n",
        "            batch_acc * 100\n",
        "        ), end=\"\")\n",
        "\n",
        "    # Update the metrics dictionary\n",
        "    metrics[\"loss\"][epoch] = np.mean(train_loss)\n",
        "    metrics[\"acc\"][epoch] = np.mean(train_acc)\n",
        "    metrics[\"confusion\"][epoch] = confusion\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def evaluate(model, val_epoch, data_loader, use_gpu, metrics):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the specified dataset.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to evaluate.\n",
        "        data_loader (torch.utils.data.DataLoader): The dataset to evaluate on.\n",
        "        use_gpu (bool): If True, attempt to train on the GPU.\n",
        "        metrics (dict): The dict where the metrics are stored.\n",
        "\n",
        "    Returns:\n",
        "        dict: The updated metrics dictionary.\n",
        "\n",
        "    \"\"\"\n",
        "    # Prepare the metrics dictionary\n",
        "    test_loss = []\n",
        "    test_acc = []\n",
        "    confusion = np.zeros((4, 4), dtype=int)\n",
        "\n",
        "    # Set the model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Iterate over test batches\n",
        "    for i, (inputs, targets) in enumerate(data_loader):\n",
        "        # Prepare the inputs\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "\n",
        "        # Forward propagation\n",
        "        with torch.no_grad():\n",
        "            logits = model(inputs)\n",
        "\n",
        "        # Loss computation\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        # Compute the total number of correct classifications\n",
        "        _, predicted = torch.max(logits, 1)\n",
        "        batch_size = inputs.size(0)\n",
        "        num_correct = (predicted == targets).sum().item()\n",
        "        batch_acc = num_correct / batch_size\n",
        "\n",
        "        # Update metrics\n",
        "        test_loss.append(loss.item())\n",
        "        test_acc.append(batch_acc)\n",
        "        confusion += confusion_matrix(targets.cpu().numpy(), predicted.cpu().numpy(), labels=[0, 1, 2, 3])\n",
        "\n",
        "        # Print the results on the console\n",
        "        print(\"\\r[Batch {} / {}] [Loss: {:.4f}] [Acc: {:.2f}%]\".format(\n",
        "            i + 1,\n",
        "            len(data_loader),\n",
        "            loss.item(),\n",
        "            batch_acc * 100\n",
        "        ), end=\"\")\n",
        "\n",
        "    # Update the metrics dictionary\n",
        "    metrics[\"loss\"][val_epoch] = np.mean(test_loss)\n",
        "    metrics[\"acc\"][val_epoch] = np.mean(test_acc)\n",
        "    metrics[\"confusion\"][val_epoch] = confusion\n",
        "\n",
        "    return metrics"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oaDxEoSCuOS"
      },
      "source": [
        "!rm -rf data/.ipynb_checkpoints/"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDyWL7tSBwpz",
        "outputId": "525d08ff-bc34-4e8f-ce6a-47d7fd4e0dc5"
      },
      "source": [
        "train_model(dataset_dir='data', use_gpu=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Using experiment name: exp_000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========== EPOCH 1/100 ==========\n",
            "[Batch 1 / 1] [Loss: 4.4176] [Acc: 22.22%]\n",
            "\n",
            "Loss: 4.417627811431885\n",
            "Accuracy: 0.2222222222222222\n",
            "\n",
            "Confusion Matrix\n",
            "[[0 1 4 0]\n",
            " [0 0 2 0]\n",
            " [0 0 2 0]\n",
            " [0 0 0 0]]\n",
            "========== EPOCH 2/100 ==========\n",
            "[Batch 1 / 1] [Loss: 0.9727] [Acc: 66.67%]========== EPOCH 3/100 ==========\n",
            "[Batch 1 / 1] [Loss: 2.7759] [Acc: 55.56%]========== EPOCH 4/100 ==========\n",
            "[Batch 1 / 1] [Loss: 5.3284] [Acc: 55.56%]========== EPOCH 5/100 ==========\n",
            "[Batch 1 / 1] [Loss: 6.0369] [Acc: 55.56%]========== EPOCH 6/100 ==========\n",
            "[Batch 1 / 1] [Loss: 2.2790] [Acc: 55.56%]========== EPOCH 7/100 ==========\n",
            "[Batch 1 / 1] [Loss: 1.7448] [Acc: 66.67%]========== EPOCH 8/100 ==========\n",
            "[Batch 1 / 1] [Loss: 2.0488] [Acc: 66.67%]========== EPOCH 9/100 ==========\n",
            "[Batch 1 / 1] [Loss: 0.2440] [Acc: 88.89%]========== EPOCH 10/100 ==========\n",
            "[Batch 1 / 1] [Loss: 0.0986] [Acc: 88.89%]========== EPOCH 11/100 ==========\n",
            "[Batch 1 / 1] [Loss: 0.1055] [Acc: 100.00%]\n",
            "\n",
            "Loss: 0.10554042458534241\n",
            "Accuracy: 1.0\n",
            "\n",
            "Confusion Matrix\n",
            "[[5 0 0 0]\n",
            " [0 2 0 0]\n",
            " [0 0 2 0]\n",
            " [0 0 0 0]]\n",
            "========== EPOCH 12/100 ==========\n",
            "[Batch 1 / 1] [Loss: 0.2710] [Acc: 77.78%]========== EPOCH 13/100 ==========\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1W5GiPOtEJ5R"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}